# TsuruTune 2.0 - Edge Device Deep Learning Optimizer

**[English](#english) | [æ—¥æœ¬èª](#æ—¥æœ¬èª-japanese)**

---

<a name="english"></a>
## English

TsuruTune is a comprehensive deep learning model optimization tool designed for edge devices and embedded platforms. It leverages hardware acceleration (Tensor Cores, CUDA) and memory bandwidth alignment to achieve optimal performance for deep learning inference on resource-constrained devices.

## Features

### Model Optimization
- **TensorRT Integration**: Full TensorRT optimization with CUDA support
- **ONNX Runtime**: Comprehensive CPU optimization with quantization
- **Multiple Precision Formats**: FP32, FP16, BF16, INT8 support
- **Advanced Quantization**: Per-channel, symmetric, and KV-cache quantization
- **Pruning & Sparsity**: Structured and unstructured pruning patterns
- **Graph Optimizations**: Batch normalization folding, constant folding, graph fusion

### User Interface
- **Modern Electron App**: Cross-platform desktop application
- **Intuitive Dashboard**: Real-time optimization statistics and trends
- **History Management**: Complete optimization history with parameter tracking
- **Device Configuration**: Separate optimization panels for CUDA and CPU
- **Progress Tracking**: Real-time optimization progress visualization
- **Batch Optimization**: Generate multiple optimized models with different parameter combinations

### Advanced Features
- **Local Model Storage**: Organized model management with metadata
- **Optimization History**: Persistent history with rerun capabilities
- **Performance Analytics**: Detailed performance gain and memory reduction metrics with real benchmarking
- **Export Capabilities**: Save optimized models to any location, generate detailed reports, history export in JSON and CSV formats
- **GitHub Integration**: Direct access to project repository
- **16 CPU Optimization Parameters**: Complete control over quantization, pruning, graph optimizations, and runtime configuration

## Requirements

### System Requirements
- **Operating System**: Windows 10+, macOS 10.14+, Ubuntu 18.04+
- **Node.js**: Version 16.0 or higher
- **Python**: Version 3.8 or higher
- **Memory**: 4GB RAM minimum, 8GB recommended

### For CUDA Optimization (Optional)
- **NVIDIA GPU**: CUDA-compatible GPU (NVIDIA Jetson, RTX, etc.)
- **CUDA Toolkit**: Version 11.0 or higher (JetPack 5.0+ for Jetson)
- **TensorRT**: Version 8.5 or higher
- **PyTorch**: Version 2.0 or higher (use NVIDIA wheels for Jetson)

### For CPU Optimization
- **ONNX Runtime**: Automatically installed
- **NumPy**: Automatically installed

## Installation

### Quick Setup
1. **Clone the repository:**
   ```bash
   git clone https://github.com/fsudjatmiko/tsurutune-app.git
   cd tsurutune-app
   ```

2. **Install Node.js dependencies:**
   ```bash
   npm install
   ```

3. **Setup Python environment:**
   ```bash
   # On macOS/Linux
   ./setup.sh
   
   # On Windows
   setup.bat
   ```

4. **Start the application:**
   ```bash
   npm start
   ```

### Manual Python Setup
If you prefer manual setup:

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r python/requirements.txt

# For CUDA support on desktop GPUs
pip install torch torchvision tensorrt

# For Jetson devices (Orin, Xavier, Nano)
# Use NVIDIA-provided PyTorch wheels from:
# https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048
# TensorRT is pre-installed with JetPack
```

### Jetson-Specific Setup (Orin Nano, Xavier, etc.)

For NVIDIA Jetson devices with JetPack 5.0+:

```bash
# Install Node.js 18 for ARM64
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Clone and setup
git clone https://github.com/fsudjatmiko/tsurutune-app.git
cd tsurutune-app
npm install

# Create Python virtual environment
python3 -m venv venv
source venv/bin/activate

# Install CPU dependencies
pip install numpy onnx onnxruntime psutil

# Install PyTorch for Jetson (download appropriate wheel)
# Visit: https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048
# Example for JetPack 5.1:
wget https://nvidia.box.com/shared/static/[pytorch-wheel-url].whl
pip install torch-*.whl

# Install TensorRT Python bindings (TensorRT already in JetPack)
pip install pycuda

# Optional: TensorFlow for Keras models
pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v50 tensorflow

# Start the application
npm start
```

## ğŸ“– Usage Guide

### 1. Model Import
- Click "Add New Model" on the dashboard
- Select your ONNX, PyTorch (.pt/.pth), or TensorFlow (.pb) model
- The model will be imported into local storage

### 2. Optimization Configuration

#### CUDA/GPU Optimization
- **Precision**: Choose from FP32, FP16, BF16, or INT8
- **Quantization**: Configure per-channel and symmetric quantization
- **Calibration**: Provide calibration dataset for INT8
- **Pruning**: Set sparsity patterns and targets
- **Engine Settings**: Configure batch size, workspace, and tactics

#### CPU Optimization
- **Precision**: FP32, FP16, BF16, or INT8 quantization
- **Graph Optimizations**: Enable fusion, folding, batch normalization merging
- **Threading**: Configure intra-op and inter-op thread counts for optimal performance
- **Pruning**: Channel pruning, clustering, and sparsity patterns
- **Calibration**: Configurable calibration samples for accurate quantization
- **Runtime Configuration**: Batch size, execution providers, and optimization levels

### 3. Running Optimization
1. Navigate to the "Optimize" page
2. Select your target device (CUDA or CPU)
3. Configure optimization parameters
4. Click "Start Optimization"
5. Monitor real-time progress

### 4. History Management
- View all optimization attempts in the "History" page
- Filter by device, status, or date
- View detailed parameters for each optimization
- Rerun successful optimizations with the same settings
- Export history for analysis

### 5. Batch Optimization
- Navigate to the "Batch Optimize" page
- Select a model from your library
- Choose optimization variants:
  - **Precision Formats**: FP32, FP16, BF16, INT8
  - **Graph Optimizations**: Enabled/Disabled
  - **Pruning Options**: None/Light/Aggressive
- Use quick presets (All, Recommended) or custom combinations
- Start batch optimization to generate multiple optimized models
- Compare results to find the best configuration

### 6. Export & Save Models
- After optimization, click "Save to Library" to save the optimized model to any location
- Click "Generate Report" to create a detailed optimization report with metrics
- Use the file explorer dialog to choose save location

### 7. Analytics Dashboard
The dashboard provides:
- **Model Statistics**: Total models and optimizations
- **Performance Metrics**: Average gains and memory reduction with real benchmarking
- **Success Rates**: Optimization success statistics
- **Activity Feed**: Recent optimization activities
- **Device Usage**: Most used devices and precision formats

## Architecture

### Frontend (Electron)
```
src/
â”œâ”€â”€ main/           # Electron main process
â”‚   â”œâ”€â”€ main.js     # Application entry point
â”‚   â””â”€â”€ preload.js  # IPC bridge
â””â”€â”€ renderer/       # UI components
    â”œâ”€â”€ index.html  # Main interface
    â”œâ”€â”€ renderer.js # Frontend logic
    â””â”€â”€ css/        # Styling
```

### Backend (Python)
```
python/
â”œâ”€â”€ main.py              # Backend entry point
â”œâ”€â”€ model_manager.py     # Model storage management
â”œâ”€â”€ history_manager.py   # Optimization history
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ cuda_optimizer.py   # TensorRT optimization
â”‚   â””â”€â”€ cpu_optimizer.py    # ONNX Runtime optimization
â””â”€â”€ utils/
    â””â”€â”€ logger.py        # Logging utilities
```

### Communication Flow
1. **Frontend** â†’ Electron IPC â†’ **Main Process**
2. **Main Process** â†’ Python subprocess â†’ **Backend**
3. **Backend** â†’ JSON response â†’ **Main Process**
4. **Main Process** â†’ IPC response â†’ **Frontend**

## ğŸ”§ Development

### Running in Development Mode
```bash
npm run dev
```

### Building for Production
```bash
# Build for current platform
npm run build

# Build for specific platforms
npm run build:win    # Windows
npm run build:mac    # macOS
npm run build:linux  # Linux
```

### Python Backend Testing
```bash
# Test system information
python python/main.py system

# Test optimization history
python python/main.py history

# Test with configuration
python python/main.py optimize --config '{"modelPath":"/path/to/model.onnx","device":"cpu"}'
```

## Performance Benchmarks

Typical optimization results on edge devices:

| Model Type | Original Size | Optimized Size | Performance Gain | Memory Reduction |
|------------|---------------|----------------|------------------|------------------|
| ResNet-50  | 98MB         | 25MB (INT8)   | +45%            | 74%             |
| ResNet-50  | 98MB         | 49MB (FP16)   | +30%            | 50%             |
| YOLOv5     | 45MB         | 12MB (INT8)   | +60%            | 73%             |
| BERT-Base  | 110MB        | 28MB (INT8)   | +35%            | 75%             |

*Results may vary based on hardware configuration and optimization settings. Benchmarks performed using real inference timing.*

## Testing

### Running Tests
```bash
# Frontend tests
npm test

# Python backend tests
python -m pytest python/tests/

# Integration tests
npm run test:integration
```

### Code Style
- **JavaScript**: ESLint configuration included
- **Python**: Follow PEP 8 guidelines
- **Commits**: Use conventional commit messages

## Acknowledgments

- **NVIDIA** for TensorRT and CUDA technologies
- **Microsoft** for ONNX Runtime
- **Electron** for the cross-platform framework
- **Open Source Community** for various libraries and tools

## Roadmap

### Version 2.0 (Planned)
- [ ] Multi-GPU optimization support
- [ ] Custom optimization profiles
- [ ] Model comparison tools
- [ ] Cloud deployment integration
- [ ] Advanced pruning algorithms

### Version 1.1 (Current)
- [x] Complete TensorRT integration
- [x] ONNX Runtime optimization with all 16 parameters
- [x] History management system
- [x] Performance analytics with real benchmarking
- [x] Batch optimization with preset combinations
- [x] FP16/BF16 CPU optimization support
- [x] Export and save optimized models
- [x] Detailed optimization reports
- [ ] Model validation tools
- [ ] Advanced pruning algorithms

---

<a name="æ—¥æœ¬èª-japanese"></a>
## æ—¥æœ¬èª (Japanese)

TsuruTuneã¯ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¨çµ„ã¿è¾¼ã¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‘ã‘ã«è¨­è¨ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆTensor Coreã€CUDAï¼‰ã¨ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æ´»ç”¨ã—ã¦ã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®ã‚ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã§ã®æ·±å±¤å­¦ç¿’æ¨è«–ã®æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## æ©Ÿèƒ½

### ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
- **TensorRTçµ±åˆ**: CUDAã‚µãƒãƒ¼ãƒˆã«ã‚ˆã‚‹å®Œå…¨ãªTensorRTæœ€é©åŒ–
- **ONNX Runtime**: é‡å­åŒ–ã‚’å«ã‚€åŒ…æ‹¬çš„ãªCPUæœ€é©åŒ–
- **è¤‡æ•°ç²¾åº¦å½¢å¼**: FP32ã€FP16ã€BF16ã€INT8ã‚µãƒãƒ¼ãƒˆ
- **é«˜åº¦ãªé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«æ¯ã€å¯¾ç§°ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–
- **ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼†ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–**: æ§‹é€ åŒ–ãƒ»éæ§‹é€ åŒ–ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
- **ã‚°ãƒ©ãƒ•æœ€é©åŒ–**: ãƒãƒƒãƒæ­£è¦åŒ–ç•³ã¿è¾¼ã¿ã€å®šæ•°ç•³ã¿è¾¼ã¿ã€ã‚°ãƒ©ãƒ•èåˆ

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **ãƒ¢ãƒ€ãƒ³Electronã‚¢ãƒ—ãƒª**: ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- **ç›´æ„Ÿçš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–çµ±è¨ˆã¨ãƒˆãƒ¬ãƒ³ãƒ‰
- **å±¥æ­´ç®¡ç†**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½è·¡ã«ã‚ˆã‚‹å®Œå…¨ãªæœ€é©åŒ–å±¥æ­´
- **ãƒ‡ãƒã‚¤ã‚¹è¨­å®š**: CUDAã¨CPUç”¨ã®å€‹åˆ¥æœ€é©åŒ–ãƒ‘ãƒãƒ«
- **é€²æ—è¿½è·¡**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–é€²æ—å¯è¦–åŒ–
- **ãƒãƒƒãƒæœ€é©åŒ–**: ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã§è¤‡æ•°ã®æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ

### é«˜åº¦ãªæ©Ÿèƒ½
- **ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãæ•´ç†ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç®¡ç†
- **æœ€é©åŒ–å±¥æ­´**: å†å®Ÿè¡Œæ©Ÿèƒ½ä»˜ãæ°¸ç¶šå±¥æ­´
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**: å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½**: æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä»»æ„ã®å ´æ‰€ã«ä¿å­˜ã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€JSONãƒ»CSVå½¢å¼ã§ã®å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- **GitHubçµ±åˆ**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒã‚¸ãƒˆãƒªã¸ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
- **16ã®CPUæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: é‡å­åŒ–ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚°ãƒ©ãƒ•æœ€é©åŒ–ã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¨­å®šã®å®Œå…¨åˆ¶å¾¡

## å‹•ä½œè¦ä»¶

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
- **ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ **: Windows 10+ã€macOS 10.14+ã€Ubuntu 18.04+
- **Node.js**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³16.0ä»¥ä¸Š
- **Python**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³3.8ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: æœ€å°4GB RAMã€æ¨å¥¨8GB

### CUDAæœ€é©åŒ–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- **NVIDIA GPU**: CUDAå¯¾å¿œGPU
- **CUDA Toolkit**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³11.0ä»¥ä¸Š
- **TensorRT**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³8.6ä»¥ä¸Š
- **PyTorch**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³2.0ä»¥ä¸Š

### CPUæœ€é©åŒ–ç”¨
- **ONNX Runtime**: è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- **NumPy**: è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
1. **ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³:**
   ```bash
   git clone https://github.com/fsudjatmiko/tsurutune-app.git
   cd tsurutune-app
   ```

2. **Node.jsä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**
   ```bash
   npm install
   ```

3. **Pythonç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:**
   ```bash
   # macOS/Linux
   ./setup.sh
   
   # Windows
   setup.bat
   ```

4. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•:**
   ```bash
   npm start
   ```

### æ‰‹å‹•Pythonè¨­å®š
æ‰‹å‹•è¨­å®šã‚’å¸Œæœ›ã™ã‚‹å ´åˆ:

```bash
# ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r python/requirements.txt

# CUDAã‚µãƒãƒ¼ãƒˆç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install torch torchvision tensorrt
```

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### 1. ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã€Œæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
- ONNXã€PyTorchï¼ˆ.pt/.pthï¼‰ã€ã¾ãŸã¯TensorFlowï¼ˆ.pbï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
- ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™

### 2. æœ€é©åŒ–è¨­å®š

#### CUDA/GPUæœ€é©åŒ–
- **ç²¾åº¦**: FP32ã€FP16ã€BF16ã€ã¾ãŸã¯INT8ã‹ã‚‰é¸æŠ
- **é‡å­åŒ–**: ãƒãƒ£ãƒãƒ«æ¯ãŠã‚ˆã³å¯¾ç§°é‡å­åŒ–ã‚’è¨­å®š
- **ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: INT8ç”¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æä¾›
- **ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**: ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š
- **ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã€æˆ¦è¡“ã‚’è¨­å®š

#### CPUæœ€é©åŒ–
- **ç²¾åº¦**: FP32ã€FP16ã€BF16ã€ã¾ãŸã¯INT8é‡å­åŒ–
- **ã‚°ãƒ©ãƒ•æœ€é©åŒ–**: èåˆã€ç•³ã¿è¾¼ã¿ã€ãƒãƒƒãƒæ­£è¦åŒ–çµ±åˆã‚’æœ‰åŠ¹åŒ–
- **ã‚¹ãƒ¬ãƒƒãƒ‰**: æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã®intra-opãŠã‚ˆã³inter-opã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š
- **ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**: ãƒãƒ£ãƒãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
- **ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: æ­£ç¢ºãªé‡å­åŒ–ã®ãŸã‚ã®è¨­å®šå¯èƒ½ãªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒ«æ•°
- **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¨­å®š**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã€å®Ÿè¡Œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«

### 3. æœ€é©åŒ–å®Ÿè¡Œ
1. ã€Œæœ€é©åŒ–ã€ãƒšãƒ¼ã‚¸ã«ç§»å‹•
2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹ï¼ˆCUDAã¾ãŸã¯CPUï¼‰ã‚’é¸æŠ
3. æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
4. ã€Œæœ€é©åŒ–é–‹å§‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
5. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚’ç›£è¦–

### 4. å±¥æ­´ç®¡ç†
- ã€Œå±¥æ­´ã€ãƒšãƒ¼ã‚¸ã§ã™ã¹ã¦ã®æœ€é©åŒ–è©¦è¡Œã‚’è¡¨ç¤º
- ãƒ‡ãƒã‚¤ã‚¹ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ã¾ãŸã¯æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿
- å„æœ€é©åŒ–ã®è©³ç´°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
- åŒã˜è¨­å®šã§æˆåŠŸã—ãŸæœ€é©åŒ–ã‚’å†å®Ÿè¡Œ
- åˆ†æç”¨å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

### 5. ãƒãƒƒãƒæœ€é©åŒ–
- ã€Œãƒãƒƒãƒæœ€é©åŒ–ã€ãƒšãƒ¼ã‚¸ã«ç§»å‹•
- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
- æœ€é©åŒ–ãƒãƒªã‚¢ãƒ³ãƒˆã‚’é¸æŠ:
  - **ç²¾åº¦å½¢å¼**: FP32ã€FP16ã€BF16ã€INT8
  - **ã‚°ãƒ©ãƒ•æœ€é©åŒ–**: æœ‰åŠ¹/ç„¡åŠ¹
  - **ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³**: ãªã—/è»½é‡/ç©æ¥µçš„
- ã‚¯ã‚¤ãƒƒã‚¯ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆã™ã¹ã¦ã€æ¨å¥¨ï¼‰ã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ çµ„ã¿åˆã‚ã›ã‚’ä½¿ç”¨
- ãƒãƒƒãƒæœ€é©åŒ–ã‚’é–‹å§‹ã—ã¦è¤‡æ•°ã®æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ
- çµæœã‚’æ¯”è¼ƒã—ã¦æœ€é©ãªè¨­å®šã‚’è¦‹ã¤ã‘ã‚‹

### 6. ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨ä¿å­˜
- æœ€é©åŒ–å¾Œã€ã€Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¿å­˜ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä»»æ„ã®å ´æ‰€ã«ä¿å­˜
- ã€Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä»˜ãè©³ç´°æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã§ä¿å­˜å ´æ‰€ã‚’é¸æŠ

### 7. åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ä»¥ä¸‹ã‚’æä¾›:
- **ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ**: ç·ãƒ¢ãƒ‡ãƒ«æ•°ã¨æœ€é©åŒ–æ•°
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹å¹³å‡å‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **æˆåŠŸç‡**: æœ€é©åŒ–æˆåŠŸçµ±è¨ˆ
- **ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ•ã‚£ãƒ¼ãƒ‰**: æœ€è¿‘ã®æœ€é©åŒ–æ´»å‹•
- **ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨**: æœ€ã‚‚ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒã‚¤ã‚¹ã¨ç²¾åº¦å½¢å¼

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆElectronï¼‰
```
src/
â”œâ”€â”€ main/           # Electronãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹
â”‚   â”œâ”€â”€ main.js     # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”‚   â””â”€â”€ preload.js  # IPCãƒ–ãƒªãƒƒã‚¸
â””â”€â”€ renderer/       # UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    â”œâ”€â”€ index.html  # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    â”œâ”€â”€ renderer.js # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ­ã‚¸ãƒƒã‚¯
    â””â”€â”€ css/        # ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
```

### ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆPythonï¼‰
```
python/
â”œâ”€â”€ main.py              # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ model_manager.py     # ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†
â”œâ”€â”€ history_manager.py   # æœ€é©åŒ–å±¥æ­´
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ cuda_optimizer.py   # TensorRTæœ€é©åŒ–
â”‚   â””â”€â”€ cpu_optimizer.py    # ONNX Runtimeæœ€é©åŒ–
â””â”€â”€ utils/
    â””â”€â”€ logger.py        # ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
```

### é€šä¿¡ãƒ•ãƒ­ãƒ¼
1. **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰** â†’ Electron IPC â†’ **ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹**
2. **ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹** â†’ Pythonã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ â†’ **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**
3. **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰** â†’ JSONå¿œç­” â†’ **ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹**
4. **ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹** â†’ IPCå¿œç­” â†’ **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰**

## ğŸ”§ é–‹ç™º

### é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
```bash
npm run dev
```

### ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç”¨ãƒ“ãƒ«ãƒ‰
```bash
# ç¾åœ¨ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç”¨ãƒ“ãƒ«ãƒ‰
npm run build

# ç‰¹å®šãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç”¨ãƒ“ãƒ«ãƒ‰
npm run build:win    # Windows
npm run build:mac    # macOS
npm run build:linux  # Linux
```

### Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
```bash
# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ãƒ†ã‚¹ãƒˆ
python python/main.py system

# æœ€é©åŒ–å±¥æ­´ãƒ†ã‚¹ãƒˆ
python python/main.py history

# è¨­å®šä»˜ããƒ†ã‚¹ãƒˆ
python python/main.py optimize --config '{"modelPath":"/path/to/model.onnx","device":"cpu"}'
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®å…¸å‹çš„ãªæœ€é©åŒ–çµæœ:

| ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— | å…ƒã®ã‚µã‚¤ã‚º | æœ€é©åŒ–å¾Œã‚µã‚¤ã‚º | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› |
|-------------|-----------|---------------|------------------|----------|
| ResNet-50   | 98MB      | 25MB (INT8)   | +45%             | 74%      |
| ResNet-50   | 98MB      | 49MB (FP16)   | +30%             | 50%      |
| YOLOv5      | 45MB      | 12MB (INT8)   | +60%             | 73%      |
| BERT-Base   | 110MB     | 28MB (INT8)   | +35%             | 75%      |

*çµæœã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã¨æœ€é©åŒ–è¨­å®šã«ã‚ˆã‚Šç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚å®Ÿéš›ã®æ¨è«–ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã€‚*

## ãƒ†ã‚¹ãƒˆ

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
npm test

# Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
python -m pytest python/tests/

# çµ±åˆãƒ†ã‚¹ãƒˆ
npm run test:integration
```

### ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«
- **JavaScript**: ESLintè¨­å®šã‚’å«ã‚€
- **Python**: PEP 8ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã†
- **Commits**: å¾“æ¥ã®ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨

## è¬è¾

- **NVIDIA** - TensorRTã¨CUDAæŠ€è¡“
- **Microsoft** - ONNX Runtime
- **Electron** - ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **Open Source Community** - å„ç¨®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ„ãƒ¼ãƒ«

## ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ãƒãƒ¼ã‚¸ãƒ§ãƒ³2.0ï¼ˆäºˆå®šï¼‰
- [ ] ãƒãƒ«ãƒGPUæœ€é©åŒ–ã‚µãƒãƒ¼ãƒˆ
- [ ] ã‚«ã‚¹ã‚¿ãƒ æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
- [ ] ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ„ãƒ¼ãƒ«
- [ ] ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤çµ±åˆ
- [ ] é«˜åº¦ãªãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### ãƒãƒ¼ã‚¸ãƒ§ãƒ³1.1ï¼ˆç¾åœ¨ï¼‰
- [x] å®Œå…¨ãªTensorRTçµ±åˆ
- [x] å…¨16ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸONNX Runtimeæœ€é©åŒ–
- [x] å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- [x] å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- [x] ãƒ—ãƒªã‚»ãƒƒãƒˆçµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒãƒƒãƒæœ€é©åŒ–
- [x] FP16/BF16 CPUæœ€é©åŒ–ã‚µãƒãƒ¼ãƒˆ
- [x] æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨ä¿å­˜
- [x] è©³ç´°ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
- [ ] ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
- [ ] é«˜åº¦ãªãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

---
*Developed by Farrell Rafee Sudjatmiko - ITS Computer Engineering*
