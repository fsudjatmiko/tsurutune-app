# TsuruTune - Jetson Deep Learning Optimizer
# TsuruTune - Jetson Deep Learning Optimizer

![TsuruTune Logo](https://via.placeholder.com/200x100/4F46E5/FFFFFF?text=TsuruTune)

## English
TsuruTune is a comprehensive deep learning model optimization tool designed specifically for NVIDIA Jetson platforms. It leverages Tensor Core acceleration and memory bandwidth alignment to achieve optimal performance for deep learning inference on edge devices.

## æ—¥æœ¬èª
TsuruTuneã¯ã€NVIDIA Jetsonãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å°‚ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚Tensor Coreã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æ´»ç”¨ã—ã¦ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®æ·±å±¤å­¦ç¿’æ¨è«–ã®æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## ğŸš€ Features | æ©Ÿèƒ½

### Model Optimization | ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
- **TensorRT Integration**: Full TensorRT optimization with CUDA support
- **TensorRTçµ±åˆ**: CUDAã‚µãƒãƒ¼ãƒˆã«ã‚ˆã‚‹å®Œå…¨ãªTensorRTæœ€é©åŒ–
- **ONNX Runtime**: Comprehensive CPU optimization with quantization
- **ONNX Runtime**: é‡å­åŒ–ã‚’å«ã‚€åŒ…æ‹¬çš„ãªCPUæœ€é©åŒ–
- **Multiple Precision Formats**: FP32, FP16, BF16, INT8 support
- **è¤‡æ•°ç²¾åº¦å½¢å¼**: FP32ã€FP16ã€BF16ã€INT8ã‚µãƒãƒ¼ãƒˆ
- **Advanced Quantization**: Per-channel, symmetric, and KV-cache quantization
- **é«˜åº¦ãªé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«æ¯ã€å¯¾ç§°ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–
- **Pruning & Sparsity**: Structured and unstructured pruning patterns
- **ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼†ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–**: æ§‹é€ åŒ–ãƒ»éæ§‹é€ åŒ–ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
- **Graph Optimizations**: Batch normalization folding, constant folding, graph fusion
- **ã‚°ãƒ©ãƒ•æœ€é©åŒ–**: ãƒãƒƒãƒæ­£è¦åŒ–ç•³ã¿è¾¼ã¿ã€å®šæ•°ç•³ã¿è¾¼ã¿ã€ã‚°ãƒ©ãƒ•èåˆ

### User Interface | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **Modern Electron App**: Cross-platform desktop application
- **ãƒ¢ãƒ€ãƒ³Electronã‚¢ãƒ—ãƒª**: ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- **Intuitive Dashboard**: Real-time optimization statistics and trends
- **ç›´æ„Ÿçš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–çµ±è¨ˆã¨ãƒˆãƒ¬ãƒ³ãƒ‰
- **History Management**: Complete optimization history with parameter tracking
- **å±¥æ­´ç®¡ç†**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½è·¡ã«ã‚ˆã‚‹å®Œå…¨ãªæœ€é©åŒ–å±¥æ­´
- **Device Configuration**: Separate optimization panels for CUDA and CPU
- **ãƒ‡ãƒã‚¤ã‚¹è¨­å®š**: CUDAã¨CPUç”¨ã®å€‹åˆ¥æœ€é©åŒ–ãƒ‘ãƒãƒ«
- **Progress Tracking**: Real-time optimization progress visualization
- **é€²æ—è¿½è·¡**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–é€²æ—å¯è¦–åŒ–

### Advanced Features | é«˜åº¦ãªæ©Ÿèƒ½
- **Local Model Storage**: Organized model management with metadata
- **ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãæ•´ç†ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç®¡ç†
- **Optimization History**: Persistent history with rerun capabilities
- **æœ€é©åŒ–å±¥æ­´**: å†å®Ÿè¡Œæ©Ÿèƒ½ä»˜ãæ°¸ç¶šå±¥æ­´
- **Performance Analytics**: Detailed performance gain and memory reduction metrics
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**: è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **Export Capabilities**: History export in JSON and CSV formats
- **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½**: JSONãƒ»CSVå½¢å¼ã§ã®å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- **GitHub Integration**: Direct access to project repository
- **GitHubçµ±åˆ**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒã‚¸ãƒˆãƒªã¸ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹

## ğŸ“‹ Requirements | å‹•ä½œè¦ä»¶

### System Requirements | ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
- **Operating System | ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ **: Windows 10+, macOS 10.14+, Ubuntu 18.04+
- **Node.js**: Version 16.0 or higher | ãƒãƒ¼ã‚¸ãƒ§ãƒ³16.0ä»¥ä¸Š
- **Python**: Version 3.8 or higher | ãƒãƒ¼ã‚¸ãƒ§ãƒ³3.8ä»¥ä¸Š
- **Memory | ãƒ¡ãƒ¢ãƒª**: 4GB RAM minimum, 8GB recommended | æœ€å°4GB RAMã€æ¨å¥¨8GB

### For CUDA Optimization (Optional) | CUDAæœ€é©åŒ–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- **NVIDIA GPU**: CUDA-compatible GPU | CUDAå¯¾å¿œGPU
- **CUDA Toolkit**: Version 11.0 or higher | ãƒãƒ¼ã‚¸ãƒ§ãƒ³11.0ä»¥ä¸Š
- **TensorRT**: Version 8.6 or higher | ãƒãƒ¼ã‚¸ãƒ§ãƒ³8.6ä»¥ä¸Š
- **PyTorch**: Version 2.0 or higher | ãƒãƒ¼ã‚¸ãƒ§ãƒ³2.0ä»¥ä¸Š

### For CPU Optimization | CPUæœ€é©åŒ–ç”¨
- **ONNX Runtime**: Automatically installed | è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- **NumPy**: Automatically installed | è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

## ğŸ› ï¸ Installation | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### Quick Setup | ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
1. **Clone the repository | ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³:**
   ```bash
   git clone https://github.com/your-username/tsurutune-app.git
   cd tsurutune-app
   ```

2. **Install Node.js dependencies | Node.jsä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**
   ```bash
   npm install
   ```

3. **Setup Python environment | Pythonç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:**
   ```bash
   # On macOS/Linux | macOS/Linux
   ./setup.sh
   
   # On Windows
   setup.bat
   ```

4. **Start the application | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•:**
   ```bash
   npm start
   ```

### Manual Python Setup | æ‰‹å‹•Pythonè¨­å®š
If you prefer manual setup | æ‰‹å‹•è¨­å®šã‚’å¸Œæœ›ã™ã‚‹å ´åˆ:

```bash
# Create virtual environment | ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies | ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r python/requirements.txt

# For CUDA support (optional) | CUDAã‚µãƒãƒ¼ãƒˆç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install torch torchvision tensorrt
```

## ğŸ“– Usage Guide | ä½¿ç”¨æ–¹æ³•

### 1. Model Import | ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- Click "Add New Model" on the dashboard | ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã€Œæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
- Select your ONNX, PyTorch (.pt/.pth), or TensorFlow (.pb) model | ONNXã€PyTorchï¼ˆ.pt/.pthï¼‰ã€ã¾ãŸã¯TensorFlowï¼ˆ.pbï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
- The model will be imported into local storage | ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™

### 2. Optimization Configuration | æœ€é©åŒ–è¨­å®š

#### CUDA/GPU Optimization | CUDA/GPUæœ€é©åŒ–
- **Precision | ç²¾åº¦**: Choose from FP32, FP16, BF16, or INT8 | FP32ã€FP16ã€BF16ã€ã¾ãŸã¯INT8ã‹ã‚‰é¸æŠ
- **Quantization | é‡å­åŒ–**: Configure per-channel and symmetric quantization | ãƒãƒ£ãƒãƒ«æ¯ãŠã‚ˆã³å¯¾ç§°é‡å­åŒ–ã‚’è¨­å®š
- **Calibration | ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: Provide calibration dataset for INT8 | INT8ç”¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æä¾›
- **Pruning | ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**: Set sparsity patterns and targets | ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š
- **Engine Settings | ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š**: Configure batch size, workspace, and tactics | ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã€æˆ¦è¡“ã‚’è¨­å®š

#### CPU Optimization | CPUæœ€é©åŒ–
- **Precision | ç²¾åº¦**: FP32 or dynamic quantization | FP32ã¾ãŸã¯å‹•çš„é‡å­åŒ–
- **Graph Optimizations | ã‚°ãƒ©ãƒ•æœ€é©åŒ–**: Enable fusion and folding | èåˆã¨ç•³ã¿è¾¼ã¿ã‚’æœ‰åŠ¹åŒ–
- **Threading | ã‚¹ãƒ¬ãƒƒãƒ‰**: Configure thread counts for optimal performance | æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š
- **Pruning | ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**: Channel pruning and clustering options | ãƒãƒ£ãƒãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³

### 3. Running Optimization | æœ€é©åŒ–å®Ÿè¡Œ
1. Navigate to the "Optimize" page | ã€Œæœ€é©åŒ–ã€ãƒšãƒ¼ã‚¸ã«ç§»å‹•
2. Select your target device (CUDA or CPU) | ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹ï¼ˆCUDAã¾ãŸã¯CPUï¼‰ã‚’é¸æŠ
3. Configure optimization parameters | æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
4. Click "Start Optimization" | ã€Œæœ€é©åŒ–é–‹å§‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
5. Monitor real-time progress | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚’ç›£è¦–

### 4. History Management | å±¥æ­´ç®¡ç†
- View all optimization attempts in the "History" page | ã€Œå±¥æ­´ã€ãƒšãƒ¼ã‚¸ã§ã™ã¹ã¦ã®æœ€é©åŒ–è©¦è¡Œã‚’è¡¨ç¤º
- Filter by device, status, or date | ãƒ‡ãƒã‚¤ã‚¹ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ã¾ãŸã¯æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿
- View detailed parameters for each optimization | å„æœ€é©åŒ–ã®è©³ç´°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
- Rerun successful optimizations with the same settings | åŒã˜è¨­å®šã§æˆåŠŸã—ãŸæœ€é©åŒ–ã‚’å†å®Ÿè¡Œ
- Export history for analysis | åˆ†æç”¨å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

### 5. Analytics Dashboard | åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
The dashboard provides | ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ä»¥ä¸‹ã‚’æä¾›:
- **Model Statistics | ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ**: Total models and optimizations | ç·ãƒ¢ãƒ‡ãƒ«æ•°ã¨æœ€é©åŒ–æ•°
- **Performance Metrics | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: Average gains and memory reduction | å¹³å‡å‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **Success Rates | æˆåŠŸç‡**: Optimization success statistics | æœ€é©åŒ–æˆåŠŸçµ±è¨ˆ
- **Activity Feed | ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ•ã‚£ãƒ¼ãƒ‰**: Recent optimization activities | æœ€è¿‘ã®æœ€é©åŒ–æ´»å‹•
- **Device Usage | ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨**: Most used devices and precision formats | æœ€ã‚‚ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒã‚¤ã‚¹ã¨ç²¾åº¦å½¢å¼

## ğŸ—ï¸ Architecture | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Frontend (Electron) | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆElectronï¼‰
```
src/
â”œâ”€â”€ main/           # Electron main process | Electronãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹
â”‚   â”œâ”€â”€ main.js     # Application entry point | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”‚   â””â”€â”€ preload.js  # IPC bridge | IPCãƒ–ãƒªãƒƒã‚¸
â””â”€â”€ renderer/       # UI components | UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    â”œâ”€â”€ index.html  # Main interface | ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    â”œâ”€â”€ renderer.js # Frontend logic | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ­ã‚¸ãƒƒã‚¯
    â””â”€â”€ css/        # Styling | ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
```

### Backend (Python) | ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆPythonï¼‰
```
python/
â”œâ”€â”€ main.py              # Backend entry point | ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ model_manager.py     # Model storage management | ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†
â”œâ”€â”€ history_manager.py   # Optimization history | æœ€é©åŒ–å±¥æ­´
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ cuda_optimizer.py   # TensorRT optimization | TensorRTæœ€é©åŒ–
â”‚   â””â”€â”€ cpu_optimizer.py    # ONNX Runtime optimization | ONNX Runtimeæœ€é©åŒ–
â””â”€â”€ utils/
    â””â”€â”€ logger.py        # Logging utilities | ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
```

### Communication Flow | é€šä¿¡ãƒ•ãƒ­ãƒ¼
1. **Frontend | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰** â†’ Electron IPC â†’ **Main Process | ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹**
2. **Main Process | ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹** â†’ Python subprocess | Pythonã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ â†’ **Backend | ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**
3. **Backend | ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰** â†’ JSON response | JSONå¿œç­” â†’ **Main Process | ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹**
4. **Main Process | ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹** â†’ IPC response | IPCå¿œç­” â†’ **Frontend | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰**

## ğŸ”§ Development | é–‹ç™º

### Running in Development Mode | é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
```bash
npm run dev
```

### Building for Production | ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç”¨ãƒ“ãƒ«ãƒ‰
```bash
# Build for current platform | ç¾åœ¨ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç”¨ãƒ“ãƒ«ãƒ‰
npm run build

# Build for specific platforms | ç‰¹å®šãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç”¨ãƒ“ãƒ«ãƒ‰
npm run build:win    # Windows
npm run build:mac    # macOS
npm run build:linux  # Linux
```

### Python Backend Testing | Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
```bash
# Test system information | ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ãƒ†ã‚¹ãƒˆ
python python/main.py system

# Test optimization history | æœ€é©åŒ–å±¥æ­´ãƒ†ã‚¹ãƒˆ
python python/main.py history

# Test with configuration | è¨­å®šä»˜ããƒ†ã‚¹ãƒˆ
python python/main.py optimize --config '{"modelPath":"/path/to/model.onnx","device":"cpu"}'
```

## ğŸ“Š Performance Benchmarks | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Typical optimization results on NVIDIA Jetson platforms | NVIDIA Jetsonãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®å…¸å‹çš„ãªæœ€é©åŒ–çµæœ:

| Model Type | Original Size | Optimized Size | Performance Gain | Memory Reduction |
|------------|---------------|----------------|------------------|------------------|
| ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— | å…ƒã®ã‚µã‚¤ã‚º | æœ€é©åŒ–å¾Œã‚µã‚¤ã‚º | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› |
| ResNet-50  | 98MB         | 25MB          | +45%            | 74%             |
| YOLOv5     | 45MB         | 12MB          | +60%            | 73%             |
| BERT-Base  | 110MB        | 28MB          | +35%            | 75%             |

*Results may vary based on hardware configuration and optimization settings.*
*çµæœã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã¨æœ€é©åŒ–è¨­å®šã«ã‚ˆã‚Šç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚*

## ğŸ§ª Testing | ãƒ†ã‚¹ãƒˆ

### Running Tests | ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# Frontend tests | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
npm test

# Python backend tests | Pythonãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
python -m pytest python/tests/

# Integration tests | çµ±åˆãƒ†ã‚¹ãƒˆ
npm run test:integration
```

## ğŸ¤ Contributing | è²¢çŒ®

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.
è²¢çŒ®ã‚’æ­“è¿ã—ã¾ã™ï¼è©³ç´°ã¯[è²¢çŒ®ã‚¬ã‚¤ãƒ‰](CONTRIBUTING.md)ã‚’ã”è¦§ãã ã•ã„ã€‚

### Development Setup | é–‹ç™ºç’°å¢ƒè¨­å®š
1. Fork the repository | ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. Create a feature branch | æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ: `git checkout -b feature-name`
3. Make your changes and test thoroughly | å¤‰æ›´ã‚’åŠ ãˆã€ååˆ†ã«ãƒ†ã‚¹ãƒˆ
4. Submit a pull request with a clear description | æ˜ç¢ºãªèª¬æ˜ã¨ã¨ã‚‚ã«ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æå‡º

### Code Style | ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«
- **JavaScript**: ESLint configuration included | ESLintè¨­å®šã‚’å«ã‚€
- **Python**: Follow PEP 8 guidelines | PEP 8ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã†
- **Commits**: Use conventional commit messages | å¾“æ¥ã®ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨

## ğŸ“ License | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ - è©³ç´°ã¯[LICENSE](LICENSE)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ™ Acknowledgments | è¬è¾

- **NVIDIA** for TensorRT and CUDA technologies | TensorRTã¨CUDAæŠ€è¡“
- **Microsoft** for ONNX Runtime | ONNX Runtime
- **Electron** for the cross-platform framework | ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **Open Source Community** for various libraries and tools | å„ç¨®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ„ãƒ¼ãƒ«

## ğŸ—ºï¸ Roadmap | ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Version 2.0 (Planned) | ãƒãƒ¼ã‚¸ãƒ§ãƒ³2.0ï¼ˆäºˆå®šï¼‰
- [ ] Multi-GPU optimization support | ãƒãƒ«ãƒGPUæœ€é©åŒ–ã‚µãƒãƒ¼ãƒˆ
- [ ] Custom optimization profiles | ã‚«ã‚¹ã‚¿ãƒ æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
- [ ] Model comparison tools | ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ„ãƒ¼ãƒ«
- [ ] Cloud deployment integration | ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤çµ±åˆ
- [ ] Advanced pruning algorithms | é«˜åº¦ãªãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### Version 1.1 (In Progress) | ãƒãƒ¼ã‚¸ãƒ§ãƒ³1.1ï¼ˆé€²è¡Œä¸­ï¼‰
- [x] Complete TensorRT integration | å®Œå…¨ãªTensorRTçµ±åˆ
- [x] ONNX Runtime optimization | ONNX Runtimeæœ€é©åŒ–
- [x] History management system | å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- [x] Performance analytics | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- [ ] Model validation tools | ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
- [ ] Batch optimization | ãƒãƒƒãƒæœ€é©åŒ–

---

**TsuruTune** - Accelerating AI at the edge with precision and performance.
**TsuruTune** - ç²¾åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§ã‚¨ãƒƒã‚¸AIã‚’åŠ é€Ÿ

*Developed by Farrell Rafee Sudjatmiko - ITS Computer Engineering*
*é–‹ç™ºè€…: Farrell Rafee Sudjatmiko - ITS Computer Engineering*
